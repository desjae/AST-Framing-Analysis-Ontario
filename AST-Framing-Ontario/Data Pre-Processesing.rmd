---
title: "Data Pre-Processing"
author: "Jason Lam"
date: "22/06/2021"
output: pdf_document
---

# 0. Preliminaries

Clear the environment: 
```{r}
rm(list = ls())
```

Load the following packages:
```{r}
library(dplyr)
library(ggraph)
library(ggplot2)
library(igraph)
library(pdftools)
library(readr)
library(reshape2)
library(stringr)
library(text2vec)
library(textdata)
library(tidyr)
library(tidytext)
library(tm) 
library(tools)
library(topicmodels)
library(widyr)
library(word2vec)
library(wordcloud)
```

This notebook outlines the procedure for data pre-processing and cleaning for the manuscript titled _How do school travel planning stakeholders frame active school travel in Ontario, Canada?_. The paper uses data retrieved from policy documents and academic papers on the subject of active school travel to conduct topic modelling and content analysis. 

The following steps can be replicated for other case studies, or to verify the method and results of the paper.

# 1. Document Retrieval

First, assemble a collection of publicly available documents sourced online from the primary school travel planning stakeholder groups: i) English school boards; ii) municipal government; and iii) transportation consortia. Manually download all relevant webpages in PDF. 

Next, conduct a search in the Web of Science database to retrieve academic papers on the topic of active school travel. Manually download all relevant studies in PDF.

# 2. Creating a Corpus

PDF documents are brought into the corpus directly, however each page of the document is read as an individual document. For our purposes, a conversion from .PDF to .txt will allow us to keep all pages in a document together when loading it into the corpus. Make sure you have the directory path to where your documents are stored and re-assign the `pdfLocation` to that corresponding location. R uses '/' as the directory separator. 

Create an individual corpus for each of the categories of documents: academic papers, municipal webpages, transportation consortia webpages, and school board webpages.

During the conversion process, some documents will have encoding errors where specific sequences of characters, such as "fi" or "fl", are encoded as ligatures in the PDF. Those characters can be lost and their subsequent words are contain spaces or are not read properly by the code.

This function allows you to find and replace strings:
```{r}
to.plain <- function(s) {

   # 1 character substitutions
   old1 <- "šžþàáâãäåçèéêëìíîïðñòóôõöùúûüý"
   new1 <- "szyaaaaaaceeeeiiiidnooooouuuuy"
   s1 <- chartr(old1, new1, s)

   # 2 character substitutions
   old2 <- c("<U+FB03>", "<U+FB01>", "<U+FB00>", "<U+FB02>")
   new2 <- c("ffi", "fi", "ff", "fl")
   s2 <- s1
   for(i in seq_along(old2)) s2 <- gsub(old2[i], new2[i], s2, fixed = TRUE)

   s2
}
```

## 2.1 Academic Papers 

### 2.1.1 Convert single column files

Trim and convert all single column formatted papers from PDF to .txt:
```{r}
pdfLocation <- "Texts/Academic Papers Trimmed Single Col"

# Create a folder for .txt files
if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }

# Create a complete list of full directory paths and file names (with extension .pdf) from the PDF location as specified above
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content

# `file_path_sans_ext(basename())` from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 

# Use of `sep = ""` argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")

write(test, file = newFile)

n <- n+1
}
```

### 2.1.2 Convert double column files

Trim and convert all double column formatted papers from PDF to .txt:
```{r}

trim <- function (x) gsub("^\\s+|\\s+$", "", x)

QTD_COLUMNS <- 2
read_text <- function(text) {
  result <- ''
  #Get all index of " " from page.
  lstops <- gregexpr(pattern =" ",text)
  #Puts the index of the most frequents ' ' in a vector.
  stops <- as.integer(names(sort(table(unlist(lstops)),decreasing=TRUE)[1:2]))
  #Slice based in the specified number of colums (this can be improved)
  for(i in seq(1, QTD_COLUMNS, by=1))
  {
    temp_result <- sapply(text, function(x){
      start <- 1
      stop <-stops[i] 
      if(i > 1)            
        start <- stops[i-1] + 1
      if(i == QTD_COLUMNS)#last column, read until end.
        stop <- nchar(x)+1
      substr(x, start=start, stop=stop)
    }, USE.NAMES=FALSE)
    temp_result <- trim(temp_result)
    result <- append(result, temp_result)
  }
  result
}

# Create a loop of the code below to run through list of documents

pdfLocation <- "Texts/Academic Papers Trimmed Double Col"
  
 if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }
  fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
  filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
  
for (pdf in filePath)
  {
txt <- pdf_text(pdf)
result <- ''
for (i in 1:length(txt)) { 
  page <- txt[i]
  t1 <- unlist(strsplit(page, "\n"))      
  maxSize <- max(nchar(t1))
  t1 <- paste0(t1,strrep(" ", maxSize-nchar(t1)))
  result = append(result,read_text(t1))
}
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(pdf)), ".txt", sep = "")
write(result, newFile)
}

```

### 2.1.3 Manually clean files

Proceed to manually cleaning the trimmed and converted .txt files by removing any remaining tables, figures, references, headers/footings, and captions that could not be trimmed. Manual corrections may be required for certain pages in academic papers that remained in two-column format after the conversion process. This typically occurs on pages that have a table or figure which disrupted the text. Finally, review all of the documents to remove hyphenation by line breaks and to keep hyphenated words together on the same line. Any ligatures (e.g., combinations of characters or letters that were not properly detected during the conversion process) can be fixed by inserting the unicode sequence of character to replace the missing sequence of characters.

### 2.1.4 Import texts that were manually cleaned

Next, import texts that were manually cleaned:
```{r}
# Location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/Academic Papers"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Create a uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

### 2.1.5 Create a data frame of texts 

Creation of a data frame of texts is required prior to conversion to corpus. The data frame must contain 'doc_id' and 'text'. Additional fields may be included in the data frame, however it will be treated as metadata by the corpus. 

Create data frame:
```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turn vector into data frame 
textDF <- data.frame(doc_id, text)

```

### 2.1.6 Convert data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
academic_corpus <- VCorpus(df_source)
```

### 2.1.7 Clean corpus

Many words should be removed while cleaning the corpus to ensure that the analysis does not pick up common nouns, adjectives, or indefinite articles. Words can be removed manually by creating a vector of strings.

Identify relevant stop words to exclude from texts:
```{r}
academic_stopwords <- c("school", "transportation", "can", "use", "active", "et", "al", "travel", "study", "elsevier", "na", "sciencedirect", "journal", "transport", "mode", "modes", "children", "the", "n", "s", "schools", "also", "p", "b", "used", "number", "data", "research", "variable", "variables", "author", "authors", "researcher", "researchers", "project", "instrument", "may", "model", "studies", "one", "table", "results", "found", "likely", "using", "less", "high", "within", "associated", "however", "among", "two", "km", "eg", "m", "j", "c", "sample", "per", "three", "r", "ie", "across", "around", "well", "a", "example", "although", "t", "fig", "signiufbcant", "1/4", "ats", "d", "literature", "overall", "'", "f", "paper", "e", "therefore", "thus", "methods", "deparmant", "result", "collected", "-", "hong", "kong", "united", "states", "new", "zealand")
```

Remove stop words and punctuation:
```{r}
academic_corpus <- tm_map(academic_corpus, stripWhitespace)
academic_corpus <- tm_map(academic_corpus, content_transformer(tolower))

academic_corpus <- tm_map(academic_corpus, removeWords, academic_stopwords)
academic_corpus <- tm_map(academic_corpus, removeNumbers)

academic_corpus <- tm_map(academic_corpus, removePunctuation)
academic_corpus <- tm_map(academic_corpus, removeWords, stopwords("english"))

```

### 2.1.8 Clean environment 

```{r}
rm(list=ls(pattern = "^text"))
rm(df_source)
```

## 2.2 Municipalities 

### 2.2.1 Convert single column files

Convert all single column webpages from PDF to .txt:
```{r}
pdfLocation <- "Texts/Municipalities or Regions"

# Create a folder for .txt files
if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }

# Create a complete list of full directory paths and file names (with extension .pdf) from the pdf location as specified above
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content

# `file_path_sans_ext(basename())` from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 

# Use of `sep = ""` argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")

write(test, file = newFile)

n <- n+1
}
```

### 2.2.2 Manually clean files

Proceed to manually cleaning the trimmed and converted .txt files by removing telephone numbers, website links, hyperlinks, and any other extraneous content that does not relate specifically to active school travel or school travel planning. Finally, review all of the documents to remove hyphenation by line breaks and to keep hyphenated words together on the same line. Any ligatures (e.g., combinations of characters or letters that were not properly detected during the conversion process) can be fixed by inserting the unicode sequence of character to replace the missing sequence of characters.

### 2.2.3 Import texts that were manually cleaned

Next, import texts that were manually cleaned:
```{r}
# Location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/Municipalities or Regions"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Create a uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

### 2.2.4 Create a data frame of texts 

Creation of a data frame of texts is required prior to conversion to corpus. The data frame must contain 'doc_id' and 'text'. Additional fields may be included in the data frame, however it will be treated as metadata by the corpus. 

Create data frame:
```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turn vector into data frame 
textDF <- data.frame(doc_id, text)

```

### 2.2.5 Convert data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
municipal_corpus <- VCorpus(df_source)
```

### 2.2.6 Clean corpus

Identify relevant stop words to exclude from texts:
```{r}
municipal_stopwords <- c("york", "lane", "school", "transportation", "region", "halton", "can", "use", "guelph", "kingston", "durham", "get", "uf", "ontario", "contact", "also", "covid", "c", "e", "one", "eg", "-", "please", "canada", "n", "page", "canadian", "email", "o", "peel", "fort", "frances", "phone", "r", "ottawa", "barrie", "km", "next", "per", "thunder", "simcoe", "website", "wellington", "andor", "catholic", "k", "m", "middlesex", "oxford", "perth", "po", "provincial", "tollfree", "toronto", "ufc")
```

Remove stop words and punctuation:
```{r}
municipal_corpus <- tm_map(municipal_corpus, stripWhitespace)
municipal_corpus <- tm_map(municipal_corpus, content_transformer(tolower))

municipal_corpus <- tm_map(municipal_corpus, removeWords, municipal_stopwords)
municipal_corpus <- tm_map(municipal_corpus, removeNumbers)

municipal_corpus <- tm_map(municipal_corpus, removePunctuation)
municipal_corpus <- tm_map(municipal_corpus, removeWords, stopwords("english"))

```

### 2.2.7 Clean environment 

```{r}
rm(list=ls(pattern = "^text"))
rm(df_source)
```

## 2.3 Consortia 

### 2.3.1 Convert single column files

Trim and convert all single column files from PDF to .txt:
```{r}
pdfLocation <- "Texts/Consortia"

# Create a folder for .txt files
if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }

# Create a complete list of full directory paths and file names (with extension .pdf) from the pdf location as specified above
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content

# `file_path_sans_ext(basename())` from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 

# Use of `sep = ""` argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")

write(test, file = newFile)

n <- n+1
}
```

### 2.3.2 Manually clean files

Proceed to manually cleaning the trimmed and converted .txt files by removing telephone numbers, website links, hyperlinks, and any other extraneous content that does not relate specifically to active school travel or school transportation. Finally, review all of the documents to remove hyphenation by line breaks and to keep hyphenated words together on the same line. Any ligatures (e.g., combinations of characters or letters that were not properly detected during the conversion process) can be fixed by inserting the unicode sequence of character to replace the missing sequence of characters.

### 2.3.3 Import texts that were manually cleaned

Next, import texts that were manually cleaned:
```{r}
# Location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/Consortia"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Create a uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

### 2.3.4 Create a data frame of texts 

Creation of a data frame of texts is required prior to conversion to corpus. The data frame must contain 'doc_id' and 'text'. Additional fields may be included in the data frame, however it will be treated as metadata by the corpus. 

Create data frame:
```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turn vector into data frame 
textDF <- data.frame(doc_id, text)

```

### 2.3.5 Conversion of data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
consortium_corpus <- VCorpus(df_source)
```

### 2.3.6 Clean corpus

Identify relevant stop words to exclude from texts:
```{r}
consortium_stopwords <- c("login", "css", "sir", "waterloo", "niagara", "john", "school", "transportation", "york", "ottawa", "can", "london", "provides", "elgin", "find", "make", "lord", "holy", "toronto", "get", "please", "will", "also", "copyright", "canada", "ontario", "translate", "download", "email", "one", "arthur", "de", "info", "website", "x")
```

Remove stop words and punctuation:
```{r}
consortium_corpus <- tm_map(consortium_corpus, stripWhitespace)
consortium_corpus <- tm_map(consortium_corpus, content_transformer(tolower))

consortium_corpus <- tm_map(consortium_corpus, removeWords, consortium_stopwords)
consortium_corpus <- tm_map(consortium_corpus, removeNumbers)

consortium_corpus <- tm_map(consortium_corpus, removePunctuation)
consortium_corpus <- tm_map(consortium_corpus, removeWords, stopwords("english"))

```

### 2.3.7 Clean environment 

```{r}
rm(list=ls(pattern = "^text"))
rm(df_source)
```

## 2.4 School Boards

### 2.4.1 Convert single column files

Trim and convert all single column files from PDF to .txt:
```{r}
pdfLocation <- "Texts/School Boards"

# Create a folder for .txt files
if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }

# Create a complete list of full directory paths and file names (with extension .pdf) from the pdf location as specified above
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content

# `file_path_sans_ext(basename())` from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 

# Use of `sep = ""` argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")

write(test, file = newFile)

n <- n+1
}
```

### 2.4.2 Manually clean files

Proceed to manually cleaning the trimmed and converted .txt files by removing telephone numbers, website links, hyperlinks, and any other extraneous content that does not relate specifically to active school travel or school transportation. Finally, review all of the documents to remove hyphenation by line breaks and to keep hyphenated words together on the same line. Any ligatures (e.g., combinations of characters or letters that were not properly detected during the conversion process) can be fixed by inserting the unicode sequence of character to replace the missing sequence of characters.

### 2.4.3 Import texts that were manually cleaned

Next, import texts that were manually cleaned:
```{r}
# Location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/School Boards"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

### 2.4.4 Create a data frame of texts 

Create data frame:
```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turn vector into data frame 
textDF <- data.frame(doc_id, text)

```

### 2.4.5 Convert  data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
school_corpus <- VCorpus(df_source)
```

### 2.4.6 Clean corpus 

Identify relevant stop words to exclude from texts:
```{r}
school_stopwords <- c("and", "asst", "board", "can", "cancellations", "catholic", "contact", "district", "east", "gta", "https", "login", "may", "or", "please", "school", "schools", "stp", "student", "students", "the", "transportation", "tri", "waterloo", "website", "will", "uf", "ontario", "-", "us", "north", "km", "county", "ottawa", "guelph", "ext", "phone", "e", "ps", "'s", "du", "toronto", "email", "o", "ottschoolbus", "simcoe", "canada", "careers", "covid", "facts", "ufd", "york", "pm", "ufa", "ufe", "wrdsb", "andor", "erin", "kg", "transporta", "cambridge", "httpsvimeocom", "muskoka", "stopr", "ufc", "windsor", "a", "durham", "geoquery", "hamiltonwentworth", "httpswwwwrdsbcaplanningwp", "hwdsb", "ic", "jk", "kawartha", "ng", "northeastern", "sudbury", "timiskaming", "wellingtondu")
```

Remove stop words and punctuation:
```{r}
school_corpus <- tm_map(school_corpus, stripWhitespace)
school_corpus <- tm_map(school_corpus, content_transformer(tolower))

school_corpus <- tm_map(school_corpus, removeWords, school_stopwords)
school_corpus <- tm_map(school_corpus, removeNumbers)

school_corpus <- tm_map(school_corpus, removePunctuation)
school_corpus <- tm_map(school_corpus, removeWords, stopwords("english"))

```

### 2.4.7 Clean environment 

```{r}
rm(list=ls(pattern = "^text"))
rm(df_source)
```

# 3. Combine Policy Corpora 

To compare the body of policy documents to the academic paper corpus, combine all of the municipal, consortia, and school board documents in one corpus.

## 3.1 Imports texts that were manually cleaned

```{r}
# Location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/Policy"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Create a uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

## 3.2 Create a data frame of texts 

```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turn vector into data frame 
textDF <- data.frame(doc_id, text)

```

## 3.3 Convert data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
policy_corpus <- VCorpus(df_source)
```

## 3.4. Create corpus for concordances

Before removing stop words and punctuation, create a corpus to use for identifying keywords in context. 

```{r}
raw_policy_corpus <- policy_corpus
```

## 3.4. Clean corpus

Identify relevant stop words to exclude from texts, in this all of the stop words previously identified for the other corpora:
```{r}
policy_stopwords <- union(consortium_stopwords, municipal_stopwords)
policy_stopwords <- union(policy_stopwords, school_stopwords)
```

Remove stop words and punctuation:
```{r}
policy_corpus <- tm_map(policy_corpus, stripWhitespace)
policy_corpus <- tm_map(policy_corpus, content_transformer(tolower))

policy_corpus <- tm_map(policy_corpus, removeWords,policy_stopwords)
policy_corpus <- tm_map(policy_corpus, removeNumbers)

policy_corpus <- tm_map(policy_corpus, removePunctuation)
policy_corpus <- tm_map(policy_corpus, removeWords, stopwords("english"))

```

# 4. Save and export the corpora 

```{r}
save(academic_corpus, file= "Corpus .rda/academic_corpus.rda")
save(consortium_corpus, file= "Corpus .rda/consortium_corpus.rda")
save(municipal_corpus, file= "Corpus .rda/municipal_corpus.rda")
save(school_corpus, file= "Corpus .rda/school_corpus.rda")
save(policy_corpus, file= "Corpus .rda/policy_corpus.rda")
save(raw_policy_corpus, file= "Corpus .rda/raw_policy_corpus.rda")
```
