---
title: "PDF to Corpus"
author: "Jason Lam"
date: "11/03/2021"
output:
  pdf_document: default
  word_document: default
---


# Introduction 

This notebook provides a methodology for converting portable document formats (PDFs) into a text corpus that can be used in natural language processing (NPL) analysis. The text corpus is a collection of structured texts which enables anaylsis using packages such as `topicmodels`. This notebook serves as an introduction on how to convert files and conduct a surface level analysis. 

Instructions are provided below for processing either one folder of PDF documents at a time and multiple folders simultaneously. (*Jason, you can add the content from 'Multiple PDF Files to Corpus.Rmd' to the sections below*)


# Preliminaries

Clear the environment: 
```{r}
rm(list = ls())
```

Load the following packages:
```{r}
library(tm)
library(readr)
library(tools)
library(dplyr)
library(tidytext)
library(topicmodels)
library(ggplot2)
library(wordcloud)
```

# Accessing Data: Grey Literature

Activel school travel (AST) and school travel planning (STP) initiatives typically bring together a group of stakeholders from the planning, transportation, public health, and education sectors. These stakeholders may or may not publish information about their involvement in AST and STP efforts on their respective websites or in policy documents. 

First, we do a preliminary scan of current AST and/or STP projects funded by the Ontario Active School Travel fund to determine which stakeholders are involved in such initiatives. We find that AST projects in are often led by a school board in collaboration with transportation planners, public health professionals, and regional transportation consortia. Non-profit organizations, police services, and advocacy groups are other common stakeholders who play a role in support AST and STP. 

Next, we create one folder for each of the stakeholder groups involved in AST and/or STP efforts. The folders below were used in the framing analysis; they are provided in this notebook as an example to recreate, modify, or extend our analysis. 

For these three following folders, webpages were sourced directly from stakeholder websites from three main groups: transportation consortias, municipalities or regions, and school boards. These webpages were saved as PDF documents via the web browser. Inclusion of webpages was determined based on the following considerations:

1) It was important that webpages were easy to find as the analysis pretains to how such issues are framed to the public. Our interpretation of 'easy to find' are webpages that required no more than 2-4 seperate links from the initial browser search. 
2) The analysis focuses on the province of Ontario, not a single municipality or region. For this reason, the search was based off a list of all school boards across this province. The websites of each school board were searched to determine whether they discussed AST and/or STP, which . 
3) Consortia and municipalities/regions were organized based on the school boards. 

__Transportation Consortia__ 

Transporation consortia are dedicated transportation bodies that primarly focus on providing the school busing service to families in their associated region. Pages are determined to be relevent if they specifically relate to AST or STP. 

In total, 9 webpages *Jason: did they mention AST? mention STP?*
*What proportion of consortia across the province meet the criteria? E.g., 9 out of how many?*

__Municipalities or Regions__ 

Municipalities or regions have general webpages about transportation plans, programs, and services provided, some of which are not relevent to our analysis. Only webpages specifically dicussing active travel, AST, or STP were included. Webpages were also sourced from associated public health units which are overseen by governing bodies at the municipal or regional level.

In total, 29 webpages *Jason: did they mention AST? mention STP?*
*What proportion of municipalities/regions across the province meet the criteria? E.g., 29 out of how many?*

__School Boards__

School boards generally have a single page to discuss transportation options and services for students. Documents were sourced from those webpages regardless if they mentioned STP and/or AST. 

In total, 27 webpages *Jason: did they mention AST? mention STP?*
*What proportion of municipalities/regions across the province meet the criteria? E.g., 27 out of how many?*


# Accessing Data: Research Literature

__Academic Papers__

[Insert Methodology for obtaining Academic Papers]


# Conversion Process 

The conversion process in this notebook can only be conducted on one file at a time. We select one of the files provided to use in this package as an example of the process. Alternatively, you can use a file relevant to your municipality/region. To run multiple files to create seperate corpora, refer to the section below.

## Searchable PDF files to .txt conversion 

PDF documents are brought into the corpus directly, however each page of the document is read as an invividual document. For our purposes, a conversion from PDF to .txt will allow us to keep all pages in a document together when loading it into the corpus. Make sure you have the directory path to where your documents are stored. Next, we manually created a seperate file to store the .txt files. Re-assign the 'pdfLocation' and 'txtLocation' to those corresponding locations.

```{r}
pdfLocation <- "E:/Dr. Paez Articles/with References"
txtLocation <-  "E:/Dr. Paez Articles/txt"

# Creates a complete list of full directory paths and file names (with extension .pdf) from the pdf location as specified above

fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content
# "file_path_sans_ext(basename())" from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 
# use of 'sep = ""' argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")
write(test, file = newFile)
print( paste(file_path_sans_ext(basename(address)), ".txt Has been created", sep = ""))
# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(newFile, file.info(newFile)$size))
n <- n+1
}
```

## Creating data frame of texts 

Creation of a data frame of texts is required prior to conversion to corpus. The data frame must contain 'doc_id' and 'text'. Additional fields may be included in the data frame, however it will be treated as metadata by the corpus. 

```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
text[i] <- eval(parse(text = paste("text", i, sep = "")))
}

# Turns vector into data frame 
textDF <- data.frame(doc_id, text)
```

## Conversion of data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
corpus <- VCorpus(df_source)
# Example 
content(corpus[[1]])
```

## Corpus cleaning 

Many words should be removed while cleaning the corpus to ensure that the analysis does not pick up on common nouns, adjectives, or indefinite articles. Words can be removed manually by creating a vector of strings. Add strings to 'NAwords': 
```{r}
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))

NAwords <- c("school", "transportation", "can", "use", "active", "et al", "travel", "j", "behav" , "nutr", "phys", "int", "n", "elsevier", "na", "sciencedirect", "journal", "transport", "mode", "modes", "children", "s")
corpus <- tm_map(corpus, removeWords, NAwords)
corpus <- tm_map(corpus, removeNumbers)

corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# Example
content(corpus[[1]])
```

## Term Document Matrix & Document Term Matrix

[Insert brief description of this step]

```{r}
TDM <- TermDocumentMatrix(corpus)
DTM <- DocumentTermMatrix(corpus)
matrix1 <- as.matrix(TDM)
matrix2 <- as.matrix(DTM)

```

## NLP Analysis 

[Insert brief description of this step]

Latent Dirichlet Allocation (LDA) (*Jason: can you explain what this means*?)
This algorithm 
```{r}
# k represents the number of topics to be generated by the algorithm 
lda <- LDA(DTM, k = 4, control = list(seed = 1234))

topics <- tidy(lda, matrix = "beta")

# The value below determines the number of top terms
top_terms <- topics %>% 
  group_by(topic) %>% 
  top_n(15, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)
top_terms

# Beta represents the probability of 'term' being generated in topic 'n' 
# Topics are generated by the LDA algorithm, its is up to the user to interpret the context of the topics 

top_terms %>% 
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~topic, scales = "free") + 
  scale_y_reordered() +
  scale_x_continuous(limits = c(0,0.06))
  
```

## Tokenization by n-gram 

[Insert brief description of this step]

```{r}
# Converting corpus to tidytext data type 
td_corpus <- tidy(corpus)

# n can be changed 
test_bigrams <- td_corpus %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

test_trigrams <- td_corpus %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)
# Bigrams & Trigrams that are most common 

top_bigrams <- test_bigrams %>%
  count(bigram, sort = TRUE)

top_trigrams <- test_trigrams %>% 
  count(trigram, sort = TRUE)

top_bigrams
top_trigrams
```

## Using TDM to make a word cloud 

[Insert brief description of this step]

```{r}
v <- sort(rowSums(matrix1), decreasing= TRUE)
d <- data.frame(word = names(v), freq=v)
set.seed(4321)
wordcloud(d$word, d$freq, min.freq = 10, max.words = 50, random.order = FALSE, rot.per = 0.5, random.color = TRUE)
```


