---
title: "Data Pre-Processing"
author: "Jason Lam"
date: "22/06/2021"
output: pdf_document
---
# Preliminaries

Clear the environment: 
```{r}
rm(list = ls())
```

Load the following packages:
```{r}
library(dplyr)
library(ggraph)
library(ggplot2)
library(igraph)
library(pdftools)
library(readr)
library(reshape2)
library(stringr)
library(text2vec)
library(textdata)
library(tidyr)
library(tidytext)
library(tm) 
library(tools)
library(topicmodels)
library(widyr)
library(word2vec)
library(wordcloud)
```

The function below allows you to find and replace certain strings. The main use in this context is to replace encoding errors where specific sequences of character such as "fi" or "fl" are encoded as ligatures in the pdf. During the conversion process those character can be lost and their subsequent words are misintrupted. 

```{r}
to.plain <- function(s) {

   # 1 character substitutions
   old1 <- "šžþàáâãäåçèéêëìíîïðñòóôõöùúûüý"
   new1 <- "szyaaaaaaceeeeiiiidnooooouuuuy"
   s1 <- chartr(old1, new1, s)

   # 2 character substitutions
   old2 <- c("<U+FB03>", "<U+FB01>", "<U+FB00>", "<U+FB02>")
   new2 <- c("ffi", "fi", "ff", "fl")
   s2 <- s1
   for(i in seq_along(old2)) s2 <- gsub(old2[i], new2[i], s2, fixed = TRUE)

   s2
}
```

Stop words 

```{r}
AP_stopW <- c("school", "transportation", "can", "use", "active", "et", "al", "travel", "study", "elsevier", "na", "sciencedirect", "journal", "transport", "mode", "modes", "children", "the", "n", "s", "schools", "also", "p", "b", "used", "number", "data", "research", "variable", "variables", "author", "authors", "researcher", "researchers", "project", "instrument", "may", "model", "studies", "one", "table", "results", "found", "likely", "using", "less", "high", "within", "associated", "however", "among", "two", "km", "eg", "m", "j", "c", "sample", "per", "three", "r", "ie", "across", "around", "well", "a", "example", "although", "t", "fig", "signiufbcant", "1/4", "ats", "d", "literature", "overall", "'", "f", "paper", "e", "therefore", "thus", "methods", "deparmant", "result", "collected", "-")

Con_stopW <- c("login", "css", "sir", "waterloo", "niagara", "john", "school", "transportation", "york", "ottawa", "can", "london", "provides", "elgin", "find", "make", "lord", "holy", "toronto", "get", "please", "will", "also", "copyright", "canada", "ontario", "translate", "download", "email", "one", "arthur", "de", "info", "website", "x")

 Mun_Reg_stopW <- c("york", "lane", "school", "transportation", "region", "halton", "can", "use", "guelph", "kingston", "durham", "get", "uf", "ontario", "contact", "also", "covid", "c", "e", "one", "eg", "-", "please", "canada", "n", "page", "canadian", "email", "o", "peel", "fort", "frances", "phone", "r", "ottawa", "barrie", "km", "next", "per", "thunder", "simcoe", "website", "wellington", "andor", "catholic", "k", "m", "middlesex", "oxford", "perth", "po", "provincial", "tollfree", "toronto", "ufc")

SB_stopW <- c("and", "asst", "board", "can", "cancellations", "catholic", "contact", "district", "east", "gta", "https", "login", "may", "or", "please", "school", "schools", "stp", "student", "students", "the", "transportation", "tri", "waterloo", "website", "will", "uf", "ontario", "-", "us", "north", "km", "county", "ottawa", "guelph", "ext", "phone", "e", "ps", "'s", "du", "toronto", "email", "o", "ottschoolbus", "simcoe", "canada", "careers", "covid", "facts", "ufd", "york", "pm", "ufa", "ufe", "wrdsb", "andor", "erin", "kg", "transporta", "cambridge", "httpsvimeocom", "muskoka", "stopr", "ufc", "windsor", "a", "durham", "geoquery", "hamiltonwentworth", "httpswwwwrdsbcaplanningwp", "hwdsb", "ic", "jk", "kawartha", "ng", "northeastern", "sudbury", "timiskaming", "wellingtondu")

Policy_StopW <- union(AP_stopW, Mun_Reg_stopW)
Policy_StopW <- union(Policy_StopW, SB_stopW)

```

# Creating Academic Corpus 

PDF documents are brought into the corpus directly, however each page of the document is read as an individual document. For our purposes, a conversion from PDF to .txt will allow us to keep all pages in a document together when loading it into the corpus. Make sure you have the directory path to where your documents are stored and re-assign the `pdfLocation` to that corresponding location. R uses '/' as the directory separator. 


## Single Column Conversion

```{r}
pdfLocation <- "Texts/Academic Papers Trimmed Single Col"

# Creates folder for .txt files
if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }

# Creates a complete list of full directory paths and file names (with extension .pdf) from the pdf location as specified above
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content


# `file_path_sans_ext(basename())` from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 

# use of `sep = ""` argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")

write(test, file = newFile)

n <- n+1
}
```

## Double Column Conversion 

```{r}

trim <- function (x) gsub("^\\s+|\\s+$", "", x)

QTD_COLUMNS <- 2
read_text <- function(text) {
  result <- ''
  #Get all index of " " from page.
  lstops <- gregexpr(pattern =" ",text)
  #Puts the index of the most frequents ' ' in a vector.
  stops <- as.integer(names(sort(table(unlist(lstops)),decreasing=TRUE)[1:2]))
  #Slice based in the specified number of colums (this can be improved)
  for(i in seq(1, QTD_COLUMNS, by=1))
  {
    temp_result <- sapply(text, function(x){
      start <- 1
      stop <-stops[i] 
      if(i > 1)            
        start <- stops[i-1] + 1
      if(i == QTD_COLUMNS)#last column, read until end.
        stop <- nchar(x)+1
      substr(x, start=start, stop=stop)
    }, USE.NAMES=FALSE)
    temp_result <- trim(temp_result)
    result <- append(result, temp_result)
  }
  result
}

# Create a loop of the code below to run through list of documents

pdfLocation <- "Texts/Academic Papers Trimmed Double Col"
  
 if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }
  fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
  filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
  
for (pdf in filePath)
  {
txt <- pdf_text(pdf)
result <- ''
for (i in 1:length(txt)) { 
  page <- txt[i]
  t1 <- unlist(strsplit(page, "\n"))      
  maxSize <- max(nchar(t1))
  t1 <- paste0(t1,strrep(" ", maxSize-nchar(t1)))
  result = append(result,read_text(t1))
}
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(pdf)), ".txt", sep = "")
write(result, newFile)
}

```

## Imports texts that were manually cleaned

```{r}
# location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/Academic Papers"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

## Creating data frame of texts 

Creation of a data frame of texts is required prior to conversion to corpus. The data frame must contain 'doc_id' and 'text'. Additional fields may be included in the data frame, however it will be treated as metadata by the corpus. 

```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turns vector into data frame 
textDF <- data.frame(doc_id, text)

```

## Conversion of data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
academic_corpus <- VCorpus(df_source)
```

## Corpus cleaning 

Many words should be removed while cleaning the corpus to ensure that the analysis does not pick up on common nouns, adjectives, or indefinite articles. Words can be removed manually by creating a vector of strings. The Argument 'AP_stopW' on line 224 accomplishes that. Refer to line 63 for stop words.

```{r}
academic_corpus <- tm_map(academic_corpus, stripWhitespace)
academic_corpus <- tm_map(academic_corpus, content_transformer(tolower))

academic_corpus <- tm_map(academic_corpus, removeWords, AP_stopW)
academic_corpus <- tm_map(academic_corpus, removeNumbers)

academic_corpus <- tm_map(academic_corpus, removePunctuation)
academic_corpus <- tm_map(academic_corpus, removeWords, stopwords("english"))

```

## Cleaning up Environment 

```{r}
rm(list=ls(pattern = "^text"))
rm(df_source)
```

# Creating Municipal Corpus 

# Single Column Conversion

```{r}
pdfLocation <- "Texts/Municipalities or Regions"

# Creates folder for .txt files
if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }

# Creates a complete list of full directory paths and file names (with extension .pdf) from the pdf location as specified above
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content


# `file_path_sans_ext(basename())` from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 

# use of `sep = ""` argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")

write(test, file = newFile)

n <- n+1
}
```

## Imports texts that were manually cleaned

```{r}
# location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/Municipalities or Regions"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

## Creating data frame of texts 

```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turns vector into data frame 
textDF <- data.frame(doc_id, text)

```

## Conversion of data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
municipal_corpus <- VCorpus(df_source)
```

## Corpus cleaning 

```{r}
municipal_corpus <- tm_map(municipal_corpus, stripWhitespace)
municipal_corpus <- tm_map(municipal_corpus, content_transformer(tolower))

municipal_corpus <- tm_map(municipal_corpus, removeWords, Mun_Reg_stopW)
municipal_corpus <- tm_map(municipal_corpus, removeNumbers)

municipal_corpus <- tm_map(municipal_corpus, removePunctuation)
municipal_corpus <- tm_map(municipal_corpus, removeWords, stopwords("english"))

```

## Cleaning up Environment 

```{r}
rm(list=ls(pattern = "^text"))
rm(df_source)
```

# Creating Consortia Corpus 

## Single Column Conversion

```{r}
pdfLocation <- "Texts/Consortia"

# Creates folder for .txt files
if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }

# Creates a complete list of full directory paths and file names (with extension .pdf) from the pdf location as specified above
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content


# `file_path_sans_ext(basename())` from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 

# use of `sep = ""` argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")

write(test, file = newFile)

n <- n+1
}
```

## Imports texts that were manually cleaned

```{r}
# location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/Consortia"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

## Creating data frame of texts 

```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turns vector into data frame 
textDF <- data.frame(doc_id, text)

```

## Conversion of data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
consortium_corpus <- VCorpus(df_source)
```

## Corpus cleaning 

```{r}
consortium_corpus <- tm_map(consortium_corpus, stripWhitespace)
consortium_corpus <- tm_map(consortium_corpus, content_transformer(tolower))

consortium_corpus <- tm_map(consortium_corpus, removeWords, Con_stopW)
consortium_corpus <- tm_map(consortium_corpus, removeNumbers)

consortium_corpus <- tm_map(consortium_corpus, removePunctuation)
consortium_corpus <- tm_map(consortium_corpus, removeWords, stopwords("english"))

```

## Cleaning up Environment 

```{r}
rm(list=ls(pattern = "^text"))
rm(df_source)
```

# Creating School Board Corpus 

## Single Column Conversion

```{r}
pdfLocation <- "Texts/School Boards"

# Creates folder for .txt files
if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }

# Creates a complete list of full directory paths and file names (with extension .pdf) from the pdf location as specified above
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content


# `file_path_sans_ext(basename())` from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 

# use of `sep = ""` argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")

write(test, file = newFile)

n <- n+1
}
```

## Imports texts that were manually cleaned

```{r}
# location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/School Boards"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

## Creating data frame of texts 

```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turns vector into data frame 
textDF <- data.frame(doc_id, text)

```

## Conversion of data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
school_corpus <- VCorpus(df_source)
```

## Corpus cleaning 

```{r}
school_corpus <- tm_map(school_corpus, stripWhitespace)
school_corpus <- tm_map(school_corpus, content_transformer(tolower))

school_corpus <- tm_map(school_corpus, removeWords, Con_stopW)
school_corpus <- tm_map(school_corpus, removeNumbers)

school_corpus <- tm_map(school_corpus, removePunctuation)
school_corpus <- tm_map(school_corpus, removeWords, stopwords("english"))

```

## Cleaning up Environment 

```{r}
rm(list=ls(pattern = "^text"))
rm(df_source)
```

# Creating Policy Corpus 

## Imports texts that were manually cleaned

```{r}
# location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/Policy"
  
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1 
for (txt in cleantxtlist) {

# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1 
}
```

## Creating data frame of texts 

```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
  #
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}

# Turns vector into data frame 
textDF <- data.frame(doc_id, text)

```

## Conversion of data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
policy_corpus <- VCorpus(df_source)
```

## Corpus cleaning 

```{r}
policy_corpus <- tm_map(policy_corpus, stripWhitespace)
policy_corpus <- tm_map(policy_corpus, content_transformer(tolower))

policy_corpus <- tm_map(policy_corpus, removeWords,Policy_StopW)
policy_corpus <- tm_map(policy_corpus, removeNumbers)

policy_corpus <- tm_map(policy_corpus, removePunctuation)
policy_corpus <- tm_map(policy_corpus, removeWords, stopwords("english"))

```

# Exporting the Corpora 

```{r}
save(academic_corpus, file= "Corpus .rda/academic_corpus.rda")
save(consortium_corpus, file= "Corpus .rda/consortium_corpus.rda")
save(municipal_corpus, file= "Corpus .rda/municipal_corpus.rda")
save(school_corpus, file= "Corpus .rda/school_corpus.rda")
save(policy_corpus, file= "Corpus .rda/policy_corpus.rda")
```

