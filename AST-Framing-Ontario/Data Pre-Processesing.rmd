---
title: "PDF to Corpus"
author: "Jason Lam"
date: "11/03/2021"
output:
  pdf_document: default
  word_document: default
---


# Introduction 

This notebook provides a methodology for converting portable document formats (PDFs) into a text corpus that can be used in natural language processing (NPL) analysis. The text corpus is a collection of structured texts which enables anaylsis using packages such as `topicmodels`. This notebook serves as an introduction on how to convert files and conduct a surface level analysis. 

Instructions are provided below for processing a single folder of PDF documents, as a walkthough of the processing techniques used. Another set of instructions is provided at the end of this notebook that allows for the processing multiple folders simultaneously. (*Jason, you can add the content from 'Multiple PDF Files to Corpus.Rmd' to the sections below*)


# Preliminaries

Clear the environment: 
```{r}
rm(list = ls())
```

Load the following packages:
```{r}
library(tm)
library(readr)
library(tools)
library(dplyr)
library(tidytext)
library(topicmodels)
library(ggplot2)
library(wordcloud)
```

# Accessing Data: Grey Literature

Activel school travel (AST) and school travel planning (STP) initiatives typically bring together a group of stakeholders from the planning, transportation, public health, and education sectors. These stakeholders may or may not publish information about their involvement in AST and STP efforts on their respective websites or in policy documents. 

First, we do a preliminary scan of current AST and/or STP projects funded by the Ontario Active School Travel fund to determine which stakeholders are involved in such initiatives. We find that AST projects in are often led by a school board in collaboration with transportation planners, public health professionals, and regional transportation consortia. Non-profit organizations, police services, and advocacy groups are other common stakeholders who play a role in support AST and STP. 

Next, we create one folder for each of the stakeholder groups involved in AST and/or STP efforts. The folders below were used in the framing analysis; they are provided in this notebook as an example to recreate, modify, or extend our analysis. 

For these three following folders, webpages were sourced directly from stakeholder websites from three main groups: transportation consortias, municipalities or regions, and school boards. These webpages were saved as PDF documents via the web browser. Inclusion of webpages was determined based on the following considerations:

1) It was important that webpages were easy to find as the analysis pretains to how such issues are framed to the public. Our interpretation of 'easy to find' are webpages that required no more than 2-4 seperate links from the initial browser search. 
2) The analysis focuses on the province of Ontario, not a single municipality or region. For this reason, the search was based off a list of all school boards across this province. The websites of each school board were searched to determine whether they discussed AST and/or STP, which . 
3) Consortia and municipalities/regions were organized based on the school boards. 

__Transportation Consortia__ 

Transporation consortia are dedicated transportation bodies that primarly focus on providing the school busing service to families in their associated region. Pages are determined to be relevent if they specifically relate to AST or STP. 

In total, 9 webpages *Jason: did they mention AST? mention STP?*
*What proportion of consortia across the province meet the criteria? E.g., 9 out of how many?*

__Municipalities or Regions__ 

Municipalities or regions have general webpages about transportation plans, programs, and services provided, some of which are not relevent to our analysis. Only webpages specifically dicussing active travel, AST, or STP were included. Webpages were also sourced from associated public health units which are overseen by governing bodies at the municipal or regional level.

In total, 29 webpages *Jason: did they mention AST? mention STP?*
*What proportion of municipalities/regions across the province meet the criteria? E.g., 29 out of how many?*

__School Boards__

School boards generally have a single page to discuss transportation options and services for students. Documents were sourced from those webpages regardless if they mentioned STP and/or AST. 

In total, 27 webpages *Jason: did they mention AST? mention STP?*
*What proportion of municipalities/regions across the province meet the criteria? E.g., 27 out of how many?*


# Accessing Data: Research Literature

__Academic Papers__

[Insert Methodology for obtaining Academic Papers]


# Conversion Process 

The conversion process in this notebook can only be conducted on one file at a time. We select one of the files provided to use in this package as an example of the process. Alternatively, you can use a file relevant to your municipality/region. To run multiple files to create seperate corpora, refer to the section below.

## Searchable PDF files to .txt conversion 

PDF documents are brought into the corpus directly, however each page of the document is read as an invividual document. For our purposes, a conversion from PDF to .txt will allow us to keep all pages in a document together when loading it into the corpus. Make sure you have the directory path to where your documents are stored. Next, we manually created a seperate file to store the .txt files. Re-assign the 'pdfLocation' and 'txtLocation' to those corresponding locations.

```{r}
pdfLocation <- "E:/Dr. Paez Articles/with References"
txtLocation <-  "E:/Dr. Paez Articles/txt"

# Creates a complete list of full directory paths and file names (with extension .pdf) from the pdf location as specified above

fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1 

for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content
# "file_path_sans_ext(basename())" from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 
# use of 'sep = ""' argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")
write(test, file = newFile)
print( paste(file_path_sans_ext(basename(address)), ".txt Has been created", sep = ""))
# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(newFile, file.info(newFile)$size))
n <- n+1
}
```

## Creating data frame of texts 

Creation of a data frame of texts is required prior to conversion to corpus. The data frame must contain 'doc_id' and 'text'. Additional fields may be included in the data frame, however it will be treated as metadata by the corpus. 

```{r}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
text[i] <- eval(parse(text = paste("text", i, sep = "")))
}

# Turns vector into data frame 
textDF <- data.frame(doc_id, text)
```

## Conversion of data frame to corpus 

Next, convert the data frame to a corpus:
```{r}
df_source <- DataframeSource(textDF)
corpus <- VCorpus(df_source)
# Example 
content(corpus[[1]])
```

## Corpus cleaning 

Many words should be removed while cleaning the corpus to ensure that the analysis does not pick up on common nouns, adjectives, or indefinite articles. Words can be removed manually by creating a vector of strings. Add strings to 'NAwords': 
```{r}
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))

NAwords <- c("school", "transportation", "can", "use", "active", "et al", "travel", "j", "behav" , "nutr", "phys", "int", "n", "elsevier", "na", "sciencedirect", "journal", "transport", "mode", "modes", "children", "s")
corpus <- tm_map(corpus, removeWords, NAwords)
corpus <- tm_map(corpus, removeNumbers)

corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# Example
content(corpus[[1]])
```

## Term Document Matrix & Document Term Matrix

[Insert brief description of this step]

```{r}
TDM <- TermDocumentMatrix(corpus)
DTM <- DocumentTermMatrix(corpus)
matrix1 <- as.matrix(TDM)
matrix2 <- as.matrix(DTM)

```

## NLP Analysis 

[Insert brief description of this step]

Latent Dirichlet Allocation (LDA) (*Jason: can you explain what this means*?)
This algorithm 
```{r}
# k represents the number of topics to be generated by the algorithm 
lda <- LDA(DTM, k = 4, control = list(seed = 1234))

topics <- tidy(lda, matrix = "beta")

# The value below determines the number of top terms
top_terms <- topics %>% 
  group_by(topic) %>% 
  top_n(15, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)
top_terms

# Beta represents the probability of 'term' being generated in topic 'n' 
# Topics are generated by the LDA algorithm, its is up to the user to interpret the context of the topics 

top_terms %>% 
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~topic, scales = "free") + 
  scale_y_reordered() +
  scale_x_continuous(limits = c(0,0.06))
  
```

## Tokenization by n-gram 

[Insert brief description of this step]

```{r}
# Converting corpus to tidytext data type 
td_corpus <- tidy(corpus)

# n can be changed 
test_bigrams <- td_corpus %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

test_trigrams <- td_corpus %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)
# Bigrams & Trigrams that are most common 

top_bigrams <- test_bigrams %>%
  count(bigram, sort = TRUE)

top_trigrams <- test_trigrams %>% 
  count(trigram, sort = TRUE)

top_bigrams
top_trigrams
```

## Using TDM to make a word cloud 

[Insert brief description of this step]

```{r}
v <- sort(rowSums(matrix1), decreasing= TRUE)
d <- data.frame(word = names(v), freq=v)
set.seed(4321)
wordcloud(d$word, d$freq, min.freq = 10, max.words = 50, random.order = FALSE, rot.per = 0.5, random.color = TRUE)
```

# Multi-Files 

Clear Environment 
```{r}
rm(list = ls())
```

'masterFolder' will be the location of your subfolders, containing your PDFs. Unlike in 'PDF to Corpus.Rmd' you do not need to create .txt folder corresponding to each PDF folder, this is done automatically by the code. 

IMPORTANT: The names of the folders cannot contain the consecutive characters 'txt', the code filters out those folders. 
NOTE: The order of files below will also be the order the corpus are created, the index will match the corpus number 
```{r}
masterFolder <- "C:/Users/Jason/Desktop/Master PDF"


subFolders <- list.files(masterFolder, full.names=TRUE)
# if txt folder already exists from previous execution of code this ensures it is not added to subfolders 
# The limitation is that your file name, at any point cannot contain the sequence of characters "txt" as it will be flagged as a non-pdf subfolder 
subFolders <- subFolders[grep(x =list.files(masterFolder, full.names=TRUE), pattern =  "txt", invert = TRUE)]
subFolders
```

This code is a is a version of the code in the 'AST-Framing-Ontario.Rmd'. It is compressed into a single block to allow the iterative conversion each subfolder. Corpus' will be cleaned for white spaces, punctuations, English stopwords, and converted to lowercases. Any additional stopwords will need to be removed manually as the user sees fit. 
```{r}
# Iterating through each subfolders 
m <- 1
for (subFolder in subFolders){
  
  pdfLocation <- subFolder
  if (file.exists(paste(pdfLocation, "txt"))){
    txtLocation <- paste(pdfLocation, "txt")
  }else{
    dir.create(paste(pdfLocation, "txt"))
    txtLocation <- paste(pdfLocation, "txt")
  }
  fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
  filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
  n <- 1 
  

  for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content
# "file_path_sans_ext(basename())" from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt 
# use of 'sep = ""' argument is important to prevent the addition of spaces, which is the default for the paste function 
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")
write(test, file = newFile)
print( paste(file_path_sans_ext(basename(address)), ".txt Has been created", sep = ""))
# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(newFile, file.info(newFile)$size))
n <- n+1
  }
  
text <- vector()
doc_id <- c(2:n-1)

for (i in seq(1, n-1, 1))
{
text[i] <- eval(parse(text = paste("text", i, sep = "")))
}

textDF <- data.frame(doc_id, text)


temp_df_source <- DataframeSource(textDF)
tempCorpus <- VCorpus(temp_df_source)
tempCorpus <- tm_map(tempCorpus, stripWhitespace)
tempCorpus <- tm_map(tempCorpus, content_transformer(tolower))
tempCorpus <- tm_map(tempCorpus, removePunctuation)
tempCorpus <- tm_map(tempCorpus, removeWords, stopwords("english"))

assign(paste("corpus", m, sep= ""), tempCorpus)

# removed tempory texts for next iteration
m <- m + 1 
rm(list=ls(pattern = "^text"))

}
rm(temp_df_source, tempCorpus, matrix1, matrix2, address, doc_id, fileBaseNames, filePath, i, m, n, newFile, pdfLocation, subFolder, test, txtLocation)
```

Uncomment and use the code below to further clean you corpuses. Insert stopwords as list of strings in 'NAwords'.  
```{r}
#NAwords <- c()
#corpus <- tm_map(corpus, removeWords, NAwords)
#corpus <- tm_map(corpus, removeNumbers)
```

Creating Term-Document Matrix and Document-Term Matrix with the fully cleaned  corpuses 
```{r}
# n - number of corpuses
n <- 3
for (i in seq(1, n, 1))
{
tempCorpus <-  eval(parse(text = paste("corpus", toString(i), sep = "")))
TDM <- TermDocumentMatrix(tempCorpus)
DTM <- DocumentTermMatrix(tempCorpus)
matrix1 <- as.matrix(TDM)
matrix2 <- as.matrix(DTM)

assign(paste("corpus", i, "TDM", sep= ""), matrix1)
assign(paste("corpus", i, "DTM", sep= ""), matrix2)

}
```
