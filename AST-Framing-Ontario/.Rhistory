masterFolder <- "Texts"
subFolders <- list.files(masterFolder, full.names=TRUE)
# if txt folder already exists from previous execution of code this ensures it is not added to subfolders
# The limitation is that your file name, at any point cannot contain the sequence of characters "txt" as it will be flagged as a non-pdf subfolder
subFolders <- subFolders[grep(x =list.files(masterFolder, full.names=TRUE), pattern =  "txt", invert = TRUE)]
subFolders
# Stop words for each of the corpora, or based on how your directory organizes files
# Add stop words if you have more than 4 corpora
NAwords2 <- c("login", "css", "sir", "waterloo", "niagara", "john", "school", "transportation", "york", "ottawa", "can", "tip", "london", "provides", "elgin", "find", "make", "active", "travel", "lord", "holy", "toronto")
NAwords3 <- c("york", "lane", "school", "transportation", "region", "halton", "can", "use", "guelph", "kingston", "durham", "get")
NAwords4 <- c("active", "and", "asst", "board", "can", "cancellations", "catholic", "contact", "cycling", "district", "east", "gta", "https", "login", "may", "or", "please", "school", "schools", "stp", "student", "students", "the", "transportation", "tri", "walking", "waterloo", "website", "will")
NAwords1 <- c("school", "transportation", "can", "use", "active", "et", "al", "travel", "study", "elsevier", "na", "sciencedirect", "journal", "transport", "mode", "modes", "children", "the", "use", "n", "s", "schools", "also", "p", "b", "used", "number", "data", "research", "variable", "variables", "author", "authors", "researcher", "researchers", "project", "intrument", "may", "-")
corpusNum <- c()
# Iterating through each subfolders
m <- 1
for (subFolder in subFolders){
pdfLocation <- subFolder
if (file.exists(paste(pdfLocation, "txt"))){
txtLocation <- paste(pdfLocation, "txt")
}else{
dir.create(paste(pdfLocation, "txt"))
txtLocation <- paste(pdfLocation, "txt")
}
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1
for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content
# "file_path_sans_ext(basename())" from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt
# use of 'sep = ""' argument is important to prevent the addition of spaces, which is the default for the paste function
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")
write(test, file = newFile)
####print(paste(file_path_sans_ext(basename(address)), ".txt Has been created", sep = ""))
# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(newFile, file.info(newFile)$size))
n <- n+1
}
text <- vector()
doc_id <- c(2:n-1)
for (i in seq(1, n-1, 1))
{
text[i] <- eval(parse(text = paste("text", i, sep = "")))
}
textDF <- data.frame(doc_id, text)
temp_df_source <- DataframeSource(textDF)
tempCorpus <- VCorpus(temp_df_source)
# corpus cleaning
tempCorpus <- tm_map(tempCorpus, stripWhitespace)
tempCorpus <- tm_map(tempCorpus, content_transformer(tolower))
tempCorpus <- tm_map(tempCorpus, removePunctuation)
tempCorpus <- tm_map(tempCorpus, removeWords, stopwords("english"))
tempCorpus <- tm_map(tempCorpus, removeWords, eval(parse(text = paste("NAwords", m, sep = ""))))
tempCorpus <- tm_map(tempCorpus, removeNumbers)
assign(paste("cleanTextDF", m, sep = ""), data.frame(text = sapply(tempCorpus, as.character), stringsAsFactors = FALSE))
assign(paste("corpus", m, sep= ""), tempCorpus)
corpusNum <- c(corpusNum, paste(m))
# removed tempory texts for next iteration
m <- m + 1
rm(list=ls(pattern = "^text"))
}
masterFolder <- "Texts"
subFolders <- list.files(masterFolder, full.names=TRUE)
# if txt folder already exists from previous execution of code this ensures it is not added to subfolders
# The limitation is that your file name, at any point cannot contain the sequence of characters "txt" as it will be flagged as a non-pdf subfolder
subFolders <- subFolders[grep(x =list.files(masterFolder, full.names=TRUE), pattern =  "txt", invert = TRUE)]
subFolders
# Stop words for each of the corpora, or based on how your directory organizes files
# Add stop words if you have more than 4 corpora
NAwords2 <- c("login", "css", "sir", "waterloo", "niagara", "john", "school", "transportation", "york", "ottawa", "can", "tip", "london", "provides", "elgin", "find", "make", "active", "travel", "lord", "holy", "toronto")
NAwords3 <- c("york", "lane", "school", "transportation", "region", "halton", "can", "use", "guelph", "kingston", "durham", "get")
NAwords4 <- c("active", "and", "asst", "board", "can", "cancellations", "catholic", "contact", "cycling", "district", "east", "gta", "https", "login", "may", "or", "please", "school", "schools", "stp", "student", "students", "the", "transportation", "tri", "walking", "waterloo", "website", "will")
NAwords1 <- c("school", "transportation", "can", "use", "active", "et", "al", "travel", "study", "elsevier", "na", "sciencedirect", "journal", "transport", "mode", "modes", "children", "the", "use", "n", "s", "schools", "also", "p", "b", "used", "number", "data", "research", "variable", "variables", "author", "authors", "researcher", "researchers", "project", "intrument", "may", "-")
corpusNum <- c()
# Iterating through each subfolders
m <- 1
for (subFolder in subFolders){
pdfLocation <- subFolder
if (file.exists(paste(pdfLocation, "txt"))){
txtLocation <- paste(pdfLocation, "txt")
}else{
dir.create(paste(pdfLocation, "txt"))
txtLocation <- paste(pdfLocation, "txt")
}
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1
for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content
# "file_path_sans_ext(basename())" from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt
# use of 'sep = ""' argument is important to prevent the addition of spaces, which is the default for the paste function
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")
write(test, file = newFile)
####print(paste(file_path_sans_ext(basename(address)), ".txt Has been created", sep = ""))
# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(newFile, file.info(newFile)$size))
n <- n+1
}
text <- vector()
doc_id <- c(2:n-1)
for (i in seq(1, n-1, 1))
{
text[i] <- eval(parse(text = paste("text", i, sep = "")))
}
textDF <- data.frame(doc_id, text)
temp_df_source <- DataframeSource(textDF)
tempCorpus <- VCorpus(temp_df_source)
# corpus cleaning
tempCorpus <- tm_map(tempCorpus, stripWhitespace)
tempCorpus <- tm_map(tempCorpus, content_transformer(tolower))
tempCorpus <- tm_map(tempCorpus, removePunctuation)
tempCorpus <- tm_map(tempCorpus, removeWords, stopwords("english"))
tempCorpus <- tm_map(tempCorpus, removeWords, eval(parse(text = paste("NAwords", m, sep = ""))))
tempCorpus <- tm_map(tempCorpus, removeNumbers)
assign(paste("cleanTextDF", m, sep = ""), data.frame(text = sapply(tempCorpus, as.character), stringsAsFactors = FALSE))
assign(paste("corpus", m, sep= ""), tempCorpus)
corpusNum <- c(corpusNum, paste(m))
# removed tempory texts for next iteration
m <- m + 1
rm(list=ls(pattern = "^text"))
}
corpusKey <- data.frame(corpusNum, subFolders)
rm(temp_df_source, tempCorpus, address, doc_id, fileBaseNames, filePath, i, m, n, newFile, pdfLocation, subFolder, test, txtLocation)
masterFolder <- "Texts"
subFolders <- list.files(masterFolder, full.names=TRUE)
# if txt folder already exists from previous execution of code this ensures it is not added to subfolders
# The limitation is that your file name, at any point cannot contain the sequence of characters "txt" as it will be flagged as a non-pdf subfolder
subFolders <- subFolders[grep(x =list.files(masterFolder, full.names=TRUE), pattern =  "txt", invert = TRUE)]
subFolders
# Stop words for each of the corpora, or based on how your directory organizes files
# Add stop words if you have more than 4 corpora
NAwords2 <- c("login", "css", "sir", "waterloo", "niagara", "john", "school", "transportation", "york", "ottawa", "can", "tip", "london", "provides", "elgin", "find", "make", "active", "travel", "lord", "holy", "toronto")
NAwords3 <- c("york", "lane", "school", "transportation", "region", "halton", "can", "use", "guelph", "kingston", "durham", "get")
NAwords4 <- c("active", "and", "asst", "board", "can", "cancellations", "catholic", "contact", "cycling", "district", "east", "gta", "https", "login", "may", "or", "please", "school", "schools", "stp", "student", "students", "the", "transportation", "tri", "walking", "waterloo", "website", "will")
NAwords1 <- c("school", "transportation", "can", "use", "active", "et", "al", "travel", "study", "elsevier", "na", "sciencedirect", "journal", "transport", "mode", "modes", "children", "the", "use", "n", "s", "schools", "also", "p", "b", "used", "number", "data", "research", "variable", "variables", "author", "authors", "researcher", "researchers", "project", "intrument", "may", "-")
corpusNum <- c()
# Iterating through each subfolders
m <- 1
for (subFolder in subFolders){
pdfLocation <- subFolder
if (file.exists(paste(pdfLocation, "txt"))){
txtLocation <- paste(pdfLocation, "txt")
}else{
dir.create(paste(pdfLocation, "txt"))
txtLocation <- paste(pdfLocation, "txt")
}
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1
for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content
# "file_path_sans_ext(basename())" from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt
# use of 'sep = ""' argument is important to prevent the addition of spaces, which is the default for the paste function
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")
write(test, file = newFile)
####print(paste(file_path_sans_ext(basename(address)), ".txt Has been created", sep = ""))
# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(newFile, file.info(newFile)$size))
n <- n+1
}
text <- vector()
doc_id <- c(2:n-1)
for (i in seq(1, n-1, 1))
{
text[i] <- eval(parse(text = paste("text", i, sep = "")))
}
textDF <- data.frame(doc_id, text)
temp_df_source <- DataframeSource(textDF)
tempCorpus <- VCorpus(temp_df_source)
# corpus cleaning
tempCorpus <- tm_map(tempCorpus, stripWhitespace)
tempCorpus <- tm_map(tempCorpus, content_transformer(tolower))
tempCorpus <- tm_map(tempCorpus, removePunctuation)
tempCorpus <- tm_map(tempCorpus, removeWords, stopwords("english"))
tempCorpus <- tm_map(tempCorpus, removeWords, eval(parse(text = paste("NAwords", m, sep = ""))))
tempCorpus <- tm_map(tempCorpus, removeNumbers)
assign(paste("cleanTextDF", m, sep = ""), data.frame(text = sapply(tempCorpus, as.character), stringsAsFactors = FALSE))
assign(paste("corpus", m, sep= ""), tempCorpus)
corpusNum <- c(corpusNum, paste(m))
# removed tempory texts for next iteration
m <- m + 1
rm(list=ls(pattern = "^text"))
}
rm(list = ls())
masterFolder <- "Texts"
subFolders <- list.files(masterFolder, full.names=TRUE)
# if txt folder already exists from previous execution of code this ensures it is not added to subfolders
# The limitation is that your file name, at any point cannot contain the sequence of characters "txt" as it will be flagged as a non-pdf subfolder
subFolders <- subFolders[grep(x =list.files(masterFolder, full.names=TRUE), pattern =  "txt", invert = TRUE)]
subFolders
# Stop words for each of the corpora, or based on how your directory organizes files
# Add stop words if you have more than 4 corpora
NAwords2 <- c("login", "css", "sir", "waterloo", "niagara", "john", "school", "transportation", "york", "ottawa", "can", "tip", "london", "provides", "elgin", "find", "make", "active", "travel", "lord", "holy", "toronto")
NAwords3 <- c("york", "lane", "school", "transportation", "region", "halton", "can", "use", "guelph", "kingston", "durham", "get")
NAwords4 <- c("active", "and", "asst", "board", "can", "cancellations", "catholic", "contact", "cycling", "district", "east", "gta", "https", "login", "may", "or", "please", "school", "schools", "stp", "student", "students", "the", "transportation", "tri", "walking", "waterloo", "website", "will")
NAwords1 <- c("school", "transportation", "can", "use", "active", "et", "al", "travel", "study", "elsevier", "na", "sciencedirect", "journal", "transport", "mode", "modes", "children", "the", "use", "n", "s", "schools", "also", "p", "b", "used", "number", "data", "research", "variable", "variables", "author", "authors", "researcher", "researchers", "project", "intrument", "may", "-")
corpusNum <- c()
# Iterating through each subfolders
m <- 1
for (subFolder in subFolders){
pdfLocation <- subFolder
if (file.exists(paste(pdfLocation, "txt"))){
txtLocation <- paste(pdfLocation, "txt")
}else{
dir.create(paste(pdfLocation, "txt"))
txtLocation <- paste(pdfLocation, "txt")
}
fileBaseNames <- list.files(pdfLocation, pattern = "*.pdf", full.names=FALSE)
filePath <- list.files(pdfLocation, pattern = "*.pdf", full.names=TRUE)
n <- 1
for (address in filePath)
{
doc <- readPDF(control = list(text = "-layout"))(elem = list(uri = address), language = "en")
test <- doc$content
# "file_path_sans_ext(basename())" from the tools package strips the base name for the file of interest and removed the extension at the end which as seen below allow for the replacement of .pdf for .txt
# use of 'sep = ""' argument is important to prevent the addition of spaces, which is the default for the paste function
newFile <- paste(txtLocation, "/", file_path_sans_ext(basename(address)), ".txt", sep = "")
write(test, file = newFile)
####print(paste(file_path_sans_ext(basename(address)), ".txt Has been created", sep = ""))
# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(newFile, file.info(newFile)$size))
n <- n+1
}
text <- vector()
doc_id <- c(2:n-1)
for (i in seq(1, n-1, 1))
{
text[i] <- eval(parse(text = paste("text", i, sep = "")))
}
textDF <- data.frame(doc_id, text)
temp_df_source <- DataframeSource(textDF)
tempCorpus <- VCorpus(temp_df_source)
# corpus cleaning
tempCorpus <- tm_map(tempCorpus, stripWhitespace)
tempCorpus <- tm_map(tempCorpus, content_transformer(tolower))
tempCorpus <- tm_map(tempCorpus, removePunctuation)
tempCorpus <- tm_map(tempCorpus, removeWords, stopwords("english"))
tempCorpus <- tm_map(tempCorpus, removeWords, eval(parse(text = paste("NAwords", m, sep = ""))))
tempCorpus <- tm_map(tempCorpus, removeNumbers)
assign(paste("cleanTextDF", m, sep = ""), data.frame(text = sapply(tempCorpus, as.character), stringsAsFactors = FALSE))
assign(paste("corpus", m, sep= ""), tempCorpus)
corpusNum <- c(corpusNum, paste(m))
# removed tempory texts for next iteration
m <- m + 1
rm(list=ls(pattern = "^text"))
}
corpusKey <- data.frame(corpusNum, subFolders)
rm(temp_df_source, tempCorpus, address, doc_id, fileBaseNames, filePath, i, m, n, newFile, pdfLocation, subFolder, test, txtLocation)
library(tm)
library(readr)
library(tools)
library(dplyr)
library(tidytext)
library(topicmodels)
library(ggplot2)
library(wordcloud)
library(word2vec)
library(pdftools)
library(reshape2)
library(text2vec)
library(dplyr)
library(ggraph)
library(ggplot2)
library(igraph)
library(pdftools)
library(readr)
library(reshape2)
library(stringr)
library(text2vec)
library(textdata)
library(tidyr)
library(tidytext)
library(tm)
library(tools)
library(topicmodels)
library(widyr)
library(word2vec)
library(wordcloud)
to.plain <- function(s) {
# 1 character substitutions
old1 <- "šžþàáâãäåçèéêëìíîïðñòóôõöùúûüý"
new1 <- "szyaaaaaaceeeeiiiidnooooouuuuy"
s1 <- chartr(old1, new1, s)
# 2 character substitutions
old2 <- c("<U+FB03>", "<U+FB01>", "<U+FB00>", "<U+FB02>")
new2 <- c("ffi", "fi", "ff", "fl")
s2 <- s1
for(i in seq_along(old2)) s2 <- gsub(old2[i], new2[i], s2, fixed = TRUE)
s2
}
# location of manually cleaned .txt documents
cleantxtLocation <- "Cleaned .txt/School Boards"
cleantxtlist <- list.files(cleantxtLocation, pattern = "*.txt", full.names=TRUE)
n <- 1
for (txt in cleantxtlist) {
# Creates uniquely id text store in 'Environment'
assign(paste("text", n, sep = ""), readChar(txt, file.info(txt)$size))
n <- n + 1
}
# This loop creates a vector of the texts
text <- vector()
doc_id <- c(2:n-1)
for (i in seq(1, n-1, 1))
{
#
text[i] <- to.plain(eval(parse(text = paste("text", i, sep = ""))))
}
# Turns vector into data frame
textDF <- data.frame(doc_id, text)
df_source <- DataframeSource(textDF)
corpus <- VCorpus(df_source)
# Example
content(corpus[[7]])
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
NAwords <- params$AP_stopW
corpus <- tm_map(corpus, removeWords, NAwords)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# Example
content(corpus[[7]])
TDM <- TermDocumentMatrix(corpus)
DTM <- DocumentTermMatrix(corpus)
matrix1 <- as.matrix(TDM)
matrix2 <- as.matrix(DTM)
# Converting corpus to tidytext data type
td_corpus <- tidy(corpus)
# n can be changed
test_bigrams <- td_corpus %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
test_trigrams <- td_corpus %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3)
# Bigrams & Trigrams that are most common
top_bigrams <- test_bigrams %>%
count(bigram, sort = TRUE)
top_trigrams <- test_trigrams %>%
count(trigram, sort = TRUE)
top_bigrams
top_trigrams
bigrams_separated <- test_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigram_counts <- bigrams_separated %>%
count(word1, word2, sort = TRUE)
# Adjust the parameter below to filter out based on number of occurence
# Error will occur if n is outide the range of occurences
bigram_graph <- bigram_counts %>%
filter(n > 150) %>%
graph_from_data_frame()
set.seed(2021)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightgreen", size = 4) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
bigrams_separated <- test_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigram_counts <- bigrams_separated %>%
count(word1, word2, sort = TRUE)
# Adjust the parameter below to filter out based on number of occurence
# Error will occur if n is outide the range of occurences
bigram_graph <- bigram_counts %>%
filter(n > 10) %>%
graph_from_data_frame()
set.seed(2021)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightgreen", size = 4) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
bing <- get_sentiments(lexicon = "nrc")
colnames(vocab)[1] <- "word"
bing <- get_sentiments(lexicon = "nrc")
colnames(vocab)[1] <- "word"
n <- 5
for (i in seq(1, n, 1))
{
tempDF <- eval(parse(text = paste("cleanTextDF", i, sep = "")))
it_train = itoken(tempDF$text, progressbar = FALSE)
assign(paste("vocab", i, sep= ""), create_vocabulary(it_train))
tempVocab <- create_vocabulary(it_train)
assign(paste("vocab", i, sep= ""), prune_vocabulary(tempVocab, term_count_min = 5))
}
for (i in seq(1, n, 1))
{
tempCorpus <-  eval(parse(text = paste("corpus", i, sep = "")))
TDM <- TermDocumentMatrix(tempCorpus)
DTM <- DocumentTermMatrix(tempCorpus)
matrix1 <- as.matrix(TDM)
matrix2 <- as.matrix(DTM)
assign(paste("corpus", i, "TDM", sep= ""), matrix1)
assign(paste("corpus", i, "DTM", sep= ""), matrix2)
}
# Create new data frame with the cleaned corpus
cleanTextDF <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = FALSE)
it_train = itoken(cleanTextDF$text, progressbar = FALSE)
vocab = create_vocabulary(it_train)
# Cut out terms that have a minimum count of 5
vocab <- prune_vocabulary(vocab, term_count_min = 5)
vocab
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it_train, vectorizer, skip_grams_window = 10)
glove = GlobalVectors$new(rank = 50, x_max = 10)
main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.001)
context = glove$components
word_vectors = main + t(context)
#safe = word_vectors["wellbeing", , drop = FALSE] - word_vectors["physical", , drop = FALSE] + word_vectors["mental", , drop = FALSE]
#cos_sim = sim2(x = word_vectors, y = safe, method = "cosine")
#head(sort(cos_sim[,1], decreasing = TRUE), 10)
library(widyr)
words <- td_corpus %>%
unnest_tokens(word, text, token = "ngrams", n = 1)
# word_count <- words %>%
#   count(word, sort = TRUE)
word_pairs <- words %>%
pairwise_count(word, id, sort = TRUE)
word_pairs %>%
filter(item1 == "traffic")
word_pairs %>%
filter(item1 %in% c("traffic", "independent", "safety", "urban", "density", "street")) %>%
group_by(item1) %>%
slice_max(n, n = 7) %>%
ungroup() %>%
mutate(item2 = reorder(item2, n)) %>%
ggplot(aes(item2, n)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
word_pairs %>%
filter(item1 %in% c("traffic", "independent", "safety", "urban", "density", "street")) %>%
group_by(item1) %>%
slice_max(n, n = 7) %>%
ungroup() %>%
mutate(item2 = reorder(item2, n)) %>%
ggplot(aes(item2, n)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
word_pairs %>%
filter(item1 %in% c("traffic", "independent", "safety", "urban")) %>%
group_by(item1) %>%
slice_max(n, n = 7) %>%
ungroup() %>%
mutate(item2 = reorder(item2, n)) %>%
ggplot(aes(item2, n)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
word_pairs %>%
filter(item1 %in% c("traffic", "independent", "safety", "urban", "density", "street")) %>%
group_by(item1) %>%
slice_max(n, n = 7) %>%
ungroup() %>%
mutate(item2 = reorder(item2, n)) %>%
ggplot(aes(item2, n)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
word_cors <- words %>%
group_by(word) %>%
filter(n() >= 150) %>%
pairwise_cor(word, id, sort = TRUE)
word_cors <- words %>%
group_by(word) %>%
filter(n() >= 150) %>%
pairwise_cor(word, id, sort = TRUE)
word_cors %>%
filter(item1 == "traffic")
word_cors <- words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, id, sort = TRUE)
word_cors %>%
filter(item1 == "traffic")
word_cors %>%
filter(item1 == "safety")
word_cors %>%
filter(item1 == "urban")
word_cors %>%
filter(item1 == "car")
word_cors %>%
filter(item1 == "transit")
word_cors %>%
filter(item1 == "independent")
word_cors %>%
filter(item1 == "transit")
word_cors %>%
filter(item1 == "street")
word_cors %>%
filter(item1 == "age")
word_cors %>%
filter(item1 == "parental")
word_cors %>%
filter(item1 %in% c("traffic", "independent", "safety", "urban", "density", "street")) %>%
group_by(item1) %>%
slice_max(correlation, n = 7) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
View(word_pairs)
